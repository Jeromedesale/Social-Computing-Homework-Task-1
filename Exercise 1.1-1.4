#Jérôme Desale
#Homework Task 1

import sqlite3
import pandas as pd
import os
os.system('clear')

#Exercice 1.1
DB_FILE = r"C:\Users\jerom\Downloads\database.sqlite"
try:
    conn = sqlite3.connect(DB_FILE)
    print("SQLite Database connection successful")
except Exception as e:
    print(f"Uh oh '{e}'")

tablenames_df = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table'", conn)
print(tablenames_df)

for table_name in tablenames_df["name"]:
    print(f"\n--- Table: {table_name} ---")
    
    cols_df = pd.read_sql_query(f"PRAGMA table_info({table_name});", conn)
    print("Columns:\n", cols_df)
    
    count_df = pd.read_sql_query(f"SELECT COUNT(*) AS n_rows FROM {table_name};", conn)
    print("Number of rows:\n", count_df)
    
    sample_df = pd.read_sql_query(f"SELECT * FROM {table_name} LIMIT 5;", conn)
    print("Sample data:\n", sample_df)

#Exercice 1.2

lurker_count_df = pd.read_sql_query("""
SELECT COUNT(*) AS lurker_count
FROM users u
WHERE u.id NOT IN (SELECT DISTINCT user_id FROM posts)
  AND u.id NOT IN (SELECT DISTINCT user_id FROM comments)
  AND u.id NOT IN (SELECT DISTINCT user_id FROM reactions);
""", conn)

print("Number of lurkers:")
print(lurker_count_df)

#Exercice 1.3
top5_engagement_df = pd.read_sql_query("""
SELECT u.id, u.username,
       COUNT(DISTINCT c.id) + COUNT(DISTINCT r.id) AS total_engagement
FROM users u
JOIN posts p ON u.id = p.user_id
LEFT JOIN comments c ON p.id = c.post_id
LEFT JOIN reactions r ON p.id = r.post_id
GROUP BY u.id, u.username
ORDER BY total_engagement DESC
LIMIT 5;
""", conn)

print("Top 5 users by engagement:")
print(top5_engagement_df)

#Exercice 1.4
spammers_df = pd.read_sql_query("""
WITH combined AS (
    SELECT user_id, content
    FROM posts
    UNION ALL
    SELECT user_id, content
    FROM comments
)
SELECT u.id, u.username, combined.content, COUNT(*) AS repeat_count
FROM combined
JOIN users u ON u.id = combined.user_id
GROUP BY u.id, u.username, combined.content
HAVING COUNT(*) >= 3
ORDER BY repeat_count DESC;
""", conn)

print("Users who repeated the same text at least 3 times (potential spammers):")
print(spammers_df)

#Homework Task 2
#Jérôme Desale

import sqlite3
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os
os.system('clear')

DB_FILE = "database.sqlite"

DB_FILE = r"C:\Users\jerom\Downloads\database.sqlite"
try:
    conn = sqlite3.connect(DB_FILE)
    print("SQLite Database connection successful")
except Exception as e:
    print(f"Uh oh '{e}'")

#Exercise 2.1
query = """
SELECT strftime('%Y', created_at) AS year, COUNT(*) AS post_count
FROM posts
GROUP BY year
ORDER BY year;
"""
df = pd.read_sql(query, conn)

df["year"] = df["year"].astype(int)

coeffs = np.polyfit(df["year"], df["post_count"], 1) 
trend = np.poly1d(coeffs)

future_years = np.arange(df["year"].max() + 1, df["year"].max() + 4)
future_preds = trend(future_years)

current_posts = df["post_count"].iloc[-1]
current_servers = 16
servers_per_post = current_servers / current_posts

future_servers = (future_preds * servers_per_post).round()

future_servers_with_redundancy = (future_servers * 1.2).round()

for year, posts, servers in zip(future_years, future_preds, future_servers_with_redundancy):
    print(f"Year {year}: ~{int(posts)} posts → {int(servers)} servers (with redundancy)")

plt.figure(figsize=(8,5))
plt.scatter(df["year"], df["post_count"], label="Historical data")
plt.plot(df["year"], trend(df["year"]), color="blue", label="Trend")
plt.plot(future_years, future_preds, color="red", linestyle="--", label="Forecast")
plt.xlabel("Year")
plt.ylabel("Number of posts")
plt.title("Post Growth Trend & Forecast")
plt.legend()
plt.show()

#Exercise 2.2

viral_posts_df = pd.read_sql_query("""
SELECT p.id AS post_id,
       u.username,
       COUNT(DISTINCT c.id) AS n_comments,
       COUNT(DISTINCT r.id) AS n_reactions,
       (COUNT(DISTINCT c.id) + COUNT(DISTINCT r.id)) AS total_engagement
FROM posts p
JOIN users u ON p.user_id = u.id
LEFT JOIN comments c ON p.id = c.post_id
LEFT JOIN reactions r ON p.id = r.post_id
GROUP BY p.id, u.username
ORDER BY total_engagement DESC
LIMIT 3;
""", conn)

print("the 3 most viral posts in the history of the platform:")
print(viral_posts_df)

#Exercise 2.3

content_lifecycle_df = pd.read_sql_query("""
WITH engagements AS (
    SELECT post_id, 
           MIN(created_at) AS first_engagement, 
           MAX(created_at) AS last_engagement
    FROM comments
    GROUP BY post_id
)
SELECT 
    AVG(julianday(first_engagement) - julianday(p.created_at)) AS avg_time_to_first_engagement,
    AVG(julianday(last_engagement) - julianday(p.created_at)) AS avg_time_to_last_engagement
FROM posts p
JOIN engagements e ON p.id = e.post_id;
""", conn)

print(content_lifecycle_df)

#Exercise 2.4
connection_df = pd.read_sql_query("""
WITH interactions AS (
    -- Comments as engagement
    SELECT c.user_id AS engager, p.user_id AS author
    FROM comments c
    JOIN posts p ON c.post_id = p.id
    UNION ALL
    -- Reactions as engagement
    SELECT r.user_id AS engager, p.user_id AS author
    FROM reactions r
    JOIN posts p ON r.post_id = p.id
),
pairwise AS (
    SELECT 
        CASE WHEN engager < author THEN engager ELSE author END AS user_a,
        CASE WHEN engager < author THEN author ELSE engager END AS user_b,
        COUNT(*) AS engagement_count
    FROM interactions
    WHERE engager != author
    GROUP BY user_a, user_b
)
SELECT user_a, user_b, engagement_count
FROM pairwise
ORDER BY engagement_count DESC
LIMIT 3;

""",conn)
print(connection_df)






# ----- Functions to be implemented are below

# Task 3
#Exercise 3.1
from typing import Tuple

URL_RE = re.compile(r"(https?://[^\s]+|www\.[^\s]+)", re.IGNORECASE)
EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}", re.IGNORECASE)
PHONE_RE = re.compile(r"(?:\+?\d{1,3}[\s.-]?)?(?:\(?\d{2,4}\)?[\s.-]?){2,4}\d{2,4}")

def _mask_word_exact(match: re.Match) -> str:
    s = match.group(0)
    return "*" * len(s)

def moderate_content(content: str) -> Tuple[str, float]:
    if not content:
        return "", 0.0

    text = content

    for w in TIER1_WORDS:
        if not w:
            continue
        pat = re.compile(rf"\b{re.escape(w)}\b", re.IGNORECASE)
        if pat.search(text):
            return "[content removed due to severe violation]", 5.0

    for phrase in TIER2_PHRASES:
        if not phrase:
            continue
        pat = re.compile(re.escape(phrase), re.IGNORECASE)
        if pat.search(text):
            return "[content removed due to spam/scam policy]", 5.0

    score = 0.0

    for w in TIER3_WORDS:
        if not w:
            continue
        pat = re.compile(rf"\b{re.escape(w)}\b", re.IGNORECASE)
        matches = list(pat.finditer(text))
        if matches:
            score += 2.0 * len(matches)
            text = pat.sub(lambda m: "*" * len(m.group(0)), text)

    url_matches = list(URL_RE.finditer(text))
    if url_matches:
        score += 2.0 * len(url_matches)
        text = URL_RE.sub("[link removed]", text)

    letters = [ch for ch in content if ch.isalpha()]
    if len(letters) > 15:
        upper = sum(1 for ch in letters if ch.isupper())
        if upper / len(letters) > 0.70:
            score += 0.5

    # Extra measure
    # Emails
    email_hits = list(EMAIL_RE.finditer(text))
    if email_hits:
        score += 1.0 * len(email_hits)
        text = EMAIL_RE.sub("[email removed]", text)

    # Phones
    def _phone_sub(m: re.Match) -> str:
        digits = re.sub(r"\D", "", m.group(0))
        return "[phone removed]" if len(digits) >= 7 else m.group(0)

    before = text
    text = PHONE_RE.sub(_phone_sub, text)

    if text != before:
        phone_count = text.count("[phone removed]")
        if phone_count > 0:
            score += 1.0 * phone_count

    return text, float(score)




# Task 3.2
import datetime as dt
from statistics import mean

def user_risk_analysis(user_id) -> float:
    """
    Risk score for a user based on Rules page:
      content_risk = (profile * 1) + (avg_post * 3) + (avg_comment * 1)
      age multiplier: <7d ×1.5; <30d ×1.2; else ×1.0
      extra: +0.5 if repeated identical content (spam-like behavior)
      capped at 5.0
    Returns a float only (the admin UI derives the label).
    """

    user = query_db("SELECT id, created_at, profile FROM users WHERE id = ?", (user_id,), one=True)
    if not user:
        return 0.0

    created_at = user["created_at"]
    if isinstance(created_at, str):
        try:
            created_dt = dt.datetime.strptime(created_at, "%Y-%m-%d %H:%M:%S")
        except ValueError:
            created_dt = dt.datetime.fromisoformat(created_at)
    else:
        created_dt = created_at
    account_age_days = (dt.datetime.now() - created_dt).days

    profile_score = moderate_content(user["profile"] or "")[1]

    post_rows = query_db("SELECT content FROM posts WHERE user_id = ?", (user_id,))
    post_scores = [moderate_content(r["content"] or "")[1] for r in post_rows]
    avg_post_score = mean(post_scores) if post_scores else 0.0

    comment_rows = query_db("SELECT content FROM comments WHERE user_id = ?", (user_id,))
    comment_scores = [moderate_content(r["content"] or "")[1] for r in comment_rows]
    avg_comment_score = mean(comment_scores) if comment_scores else 0.0

    content_risk_score = (profile_score * 1.0) + (avg_post_score * 3.0) + (avg_comment_score * 1.0)

    if account_age_days < 7:
        risk_score = content_risk_score * 1.5
    elif account_age_days < 30:
        risk_score = content_risk_score * 1.2
    else:
        risk_score = content_risk_score

    rep_posts = query_db("""
        SELECT 1
        FROM (
            SELECT content, COUNT(*) AS n
            FROM posts
            WHERE user_id = ?
            GROUP BY content
            HAVING COUNT(*) >= 3
        ) t
        LIMIT 1
    """, (user_id,), one=True)

    rep_comments = query_db("""
        SELECT 1
        FROM (
            SELECT content, COUNT(*) AS n
            FROM comments
            WHERE user_id = ?
            GROUP BY content
            HAVING COUNT(*) >= 3
        ) t
        LIMIT 1
    """, (user_id,), one=True)

    if rep_posts or rep_comments:
        risk_score += 0.5

    return float(min(risk_score, 5.0))



    #task 3.3
def recommend(user_id, filter_following):
    """
    Returns up to 5 recommended posts (reverse-chronological).
    Signals:
      - positive reactions by the user (like/love/laugh/wow)
      - authors the user follows
      - DIY topic boost (content contains 'diy', or seed users/posts from the brief)
    """
    POSITIVE = ('like', 'love', 'laugh', 'wow')

    if not user_id:
        return query_db("""
            SELECT p.id, p.content, p.created_at, u.username, u.id AS user_id
            FROM posts p JOIN users u ON p.user_id = u.id
            ORDER BY p.created_at DESC
            LIMIT 5
        """)

    seed_usernames = ('starboy99','DancingDolphin','blogger_bob')
    seed_posts = (1810, 1875, 1880, 2113)

    seed_rows = query_db(
        f"SELECT id FROM posts WHERE id IN ({','.join(['?']*len(seed_posts))})",
        seed_posts
    ) if seed_posts else []
    seed_post_ids = {r['id'] for r in (seed_rows or [])}

    seed_user_rows = query_db(
        f"SELECT id FROM users WHERE username IN ({','.join(['?']*len(seed_usernames))})",
        seed_usernames
    )
    seed_author_ids = {r['id'] for r in (seed_user_rows or [])}

    followed = query_db("SELECT followed_id AS aid FROM follows WHERE follower_id = ?", (user_id,))
    followed_ids = {r['aid'] for r in (followed or [])}

    liked_authors = query_db(f"""
        SELECT DISTINCT p.user_id AS aid
        FROM reactions r
        JOIN posts p ON p.id = r.post_id
        WHERE r.user_id = ? AND r.reaction_type IN ({",".join(["?"]*len(POSITIVE))})
    """, (user_id, *POSITIVE)) or []
    liked_author_ids = {r['aid'] for r in liked_authors}

    candidate_authors = followed_ids if filter_following else (followed_ids | liked_author_ids | seed_author_ids)

    if not candidate_authors:
        return query_db("""
            SELECT p.id, p.content, p.created_at, u.username, u.id AS user_id
            FROM posts p
            JOIN users u ON p.user_id = u.id
            LEFT JOIN (
                SELECT post_id, COUNT(*) AS cpos
                FROM reactions
                WHERE reaction_type IN ('like','love','laugh','wow')
                GROUP BY post_id
            ) pr ON pr.post_id = p.id
            WHERE p.user_id != ?
            ORDER BY p.created_at DESC, IFNULL(pr.cpos,0) DESC
            LIMIT 5
        """, (user_id,))

    author_list = list(candidate_authors)
    placeholders = ",".join(["?"] * len(author_list))

    base = query_db(f"""
        SELECT p.id, p.content, p.created_at, u.username, u.id AS user_id
        FROM posts p
        JOIN users u ON p.user_id = u.id
        WHERE p.user_id IN ({placeholders})
          AND p.user_id != ?
          AND p.id NOT IN (SELECT post_id FROM reactions WHERE user_id = ?)
        ORDER BY p.created_at DESC
        LIMIT 200
    """, (*author_list, user_id, user_id)) or []

    scored = []
    for r in base:
        s = 0
        aid = r['user_id']
        pid = r['id']
        text = (r['content'] or "").lower()

        if aid in followed_ids:      s += 2
        if aid in liked_author_ids:  s += 3
        if 'diy' in text:            s += 3
        if pid in seed_post_ids:     s += 2
        if aid in seed_author_ids:   s += 2

        scored.append((s, r['created_at'], r))

    scored.sort(key=lambda x: (x[0], x[1]), reverse=True)
    recs = [r for _,__,r in scored][:5]

    if len(recs) < 5:
        need = 5 - len(recs)
        seen = {r['id'] for r in recs}
        backfill = query_db(f"""
            SELECT p.id, p.content, p.created_at, u.username, u.id AS user_id
            FROM posts p
            JOIN users u ON p.user_id = u.id
            LEFT JOIN (
                SELECT post_id, COUNT(*) AS cpos
                FROM reactions
                WHERE reaction_type IN ('like','love','laugh','wow')
                GROUP BY post_id
            ) pr ON pr.post_id = p.id
            WHERE p.user_id != ?
              AND p.id NOT IN (SELECT post_id FROM reactions WHERE user_id = ?)
              {("AND p.user_id IN (" + placeholders + ")") if filter_following else ""}
            ORDER BY p.created_at DESC, IFNULL(pr.cpos,0) DESC
            LIMIT {need}
        """, (user_id, user_id, *author_list) if filter_following else (user_id, user_id,))
        for r in (backfill or []):
            if r['id'] not in seen:
                recs.append(r); seen.add(r['id'])
    return recs[:5]

if __name__ == '__main__':
    app.run(debug=True, port=8080)
