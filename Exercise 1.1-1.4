#Jérôme Desale
#Homework Task 1

import sqlite3
import pandas as pd
import os
os.system('clear')

#Exercice 1.1
DB_FILE = r"C:\Users\jerom\Downloads\database.sqlite"
try:
    conn = sqlite3.connect(DB_FILE)
    print("SQLite Database connection successful")
except Exception as e:
    print(f"Uh oh '{e}'")

tablenames_df = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table'", conn)
print(tablenames_df)

for table_name in tablenames_df["name"]:
    print(f"\n--- Table: {table_name} ---")
    
    cols_df = pd.read_sql_query(f"PRAGMA table_info({table_name});", conn)
    print("Columns:\n", cols_df)
    
    count_df = pd.read_sql_query(f"SELECT COUNT(*) AS n_rows FROM {table_name};", conn)
    print("Number of rows:\n", count_df)
    
    sample_df = pd.read_sql_query(f"SELECT * FROM {table_name} LIMIT 5;", conn)
    print("Sample data:\n", sample_df)

#Exercice 1.2

lurker_count_df = pd.read_sql_query("""
SELECT COUNT(*) AS lurker_count
FROM users u
WHERE u.id NOT IN (SELECT DISTINCT user_id FROM posts)
  AND u.id NOT IN (SELECT DISTINCT user_id FROM comments)
  AND u.id NOT IN (SELECT DISTINCT user_id FROM reactions);
""", conn)

print("Number of lurkers:")
print(lurker_count_df)

#Exercice 1.3
top5_engagement_df = pd.read_sql_query("""
SELECT u.id, u.username,
       COUNT(DISTINCT c.id) + COUNT(DISTINCT r.id) AS total_engagement
FROM users u
JOIN posts p ON u.id = p.user_id
LEFT JOIN comments c ON p.id = c.post_id
LEFT JOIN reactions r ON p.id = r.post_id
GROUP BY u.id, u.username
ORDER BY total_engagement DESC
LIMIT 5;
""", conn)

print("Top 5 users by engagement:")
print(top5_engagement_df)

#Exercice 1.4
spammers_df = pd.read_sql_query("""
WITH combined AS (
    SELECT user_id, content
    FROM posts
    UNION ALL
    SELECT user_id, content
    FROM comments
)
SELECT u.id, u.username, combined.content, COUNT(*) AS repeat_count
FROM combined
JOIN users u ON u.id = combined.user_id
GROUP BY u.id, u.username, combined.content
HAVING COUNT(*) >= 3
ORDER BY repeat_count DESC;
""", conn)

print("Users who repeated the same text at least 3 times (potential spammers):")
print(spammers_df)

#Homework Task 2
#Jérôme Desale

import sqlite3
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os
os.system('clear')

DB_FILE = "database.sqlite"

DB_FILE = r"C:\Users\jerom\Downloads\database.sqlite"
try:
    conn = sqlite3.connect(DB_FILE)
    print("SQLite Database connection successful")
except Exception as e:
    print(f"Uh oh '{e}'")

#Exercise 2.1
query = """
SELECT strftime('%Y', created_at) AS year, COUNT(*) AS post_count
FROM posts
GROUP BY year
ORDER BY year;
"""
df = pd.read_sql(query, conn)

df["year"] = df["year"].astype(int)

coeffs = np.polyfit(df["year"], df["post_count"], 1) 
trend = np.poly1d(coeffs)

future_years = np.arange(df["year"].max() + 1, df["year"].max() + 4)
future_preds = trend(future_years)

current_posts = df["post_count"].iloc[-1]
current_servers = 16
servers_per_post = current_servers / current_posts

future_servers = (future_preds * servers_per_post).round()

future_servers_with_redundancy = (future_servers * 1.2).round()

for year, posts, servers in zip(future_years, future_preds, future_servers_with_redundancy):
    print(f"Year {year}: ~{int(posts)} posts → {int(servers)} servers (with redundancy)")

plt.figure(figsize=(8,5))
plt.scatter(df["year"], df["post_count"], label="Historical data")
plt.plot(df["year"], trend(df["year"]), color="blue", label="Trend")
plt.plot(future_years, future_preds, color="red", linestyle="--", label="Forecast")
plt.xlabel("Year")
plt.ylabel("Number of posts")
plt.title("Post Growth Trend & Forecast")
plt.legend()
plt.show()

#Exercise 2.2

viral_posts_df = pd.read_sql_query("""
SELECT p.id AS post_id,
       u.username,
       COUNT(DISTINCT c.id) AS n_comments,
       COUNT(DISTINCT r.id) AS n_reactions,
       (COUNT(DISTINCT c.id) + COUNT(DISTINCT r.id)) AS total_engagement
FROM posts p
JOIN users u ON p.user_id = u.id
LEFT JOIN comments c ON p.id = c.post_id
LEFT JOIN reactions r ON p.id = r.post_id
GROUP BY p.id, u.username
ORDER BY total_engagement DESC
LIMIT 3;
""", conn)

print("the 3 most viral posts in the history of the platform:")
print(viral_posts_df)

#Exercise 2.3

content_lifecycle_df = pd.read_sql_query("""
WITH engagements AS (
    SELECT post_id, 
           MIN(created_at) AS first_engagement, 
           MAX(created_at) AS last_engagement
    FROM comments
    GROUP BY post_id
)
SELECT 
    AVG(julianday(first_engagement) - julianday(p.created_at)) AS avg_time_to_first_engagement,
    AVG(julianday(last_engagement) - julianday(p.created_at)) AS avg_time_to_last_engagement
FROM posts p
JOIN engagements e ON p.id = e.post_id;
""", conn)

print(content_lifecycle_df)

#Exercise 2.4
connection_df = pd.read_sql_query("""
WITH interactions AS (
    -- Comments as engagement
    SELECT c.user_id AS engager, p.user_id AS author
    FROM comments c
    JOIN posts p ON c.post_id = p.id
    UNION ALL
    -- Reactions as engagement
    SELECT r.user_id AS engager, p.user_id AS author
    FROM reactions r
    JOIN posts p ON r.post_id = p.id
),
pairwise AS (
    SELECT 
        CASE WHEN engager < author THEN engager ELSE author END AS user_a,
        CASE WHEN engager < author THEN author ELSE engager END AS user_b,
        COUNT(*) AS engagement_count
    FROM interactions
    WHERE engager != author
    GROUP BY user_a, user_b
)
SELECT user_a, user_b, engagement_count
FROM pairwise
ORDER BY engagement_count DESC
LIMIT 3;

""",conn)
print(connection_df)


# Task 3
#Exercise 3.1
import re
from typing import Tuple

URL_RE = re.compile(r"(https?://[^\s]+|www\.[^\s]+)", re.IGNORECASE)
EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}", re.IGNORECASE)
PHONE_RE = re.compile(r"(?:\+?\d{1,3}[\s.-]?)?(?:\(?\d{2,4}\)?[\s.-]?){2,4}\d{2,4}")

def _mask_word_exact(match: re.Match) -> str:
    s = match.group(0)
    return "*" * len(s)

def moderate_content(content: str) -> Tuple[str, float]:
    if not content:
        return "", 0.0

    text = content

    for w in TIER1_WORDS:
        if not w:
            continue
        pat = re.compile(rf"\b{re.escape(w)}\b", re.IGNORECASE)
        if pat.search(text):
            return "[content removed due to severe violation]", 5.0

    for phrase in TIER2_PHRASES:
        if not phrase:
            continue
        pat = re.compile(re.escape(phrase), re.IGNORECASE)
        if pat.search(text):
            return "[content removed due to spam/scam policy]", 5.0

    score = 0.0

    for w in TIER3_WORDS:
        if not w:
            continue
        pat = re.compile(rf"\b{re.escape(w)}\b", re.IGNORECASE)
        matches = list(pat.finditer(text))
        if matches:
            score += 2.0 * len(matches)
            text = pat.sub(lambda m: "*" * len(m.group(0)), text)

    url_matches = list(URL_RE.finditer(text))
    if url_matches:
        score += 2.0 * len(url_matches)
        text = URL_RE.sub("[link removed]", text)

    letters = [ch for ch in content if ch.isalpha()]
    if len(letters) > 15:
        upper = sum(1 for ch in letters if ch.isupper())
        if upper / len(letters) > 0.70:
            score += 0.5

    # Extra measure
    # Emails
    email_hits = list(EMAIL_RE.finditer(text))
    if email_hits:
        score += 1.0 * len(email_hits)
        text = EMAIL_RE.sub("[email removed]", text)

    # Phones
    def _phone_sub(m: re.Match) -> str:
        digits = re.sub(r"\D", "", m.group(0))
        return "[phone removed]" if len(digits) >= 7 else m.group(0)

    before = text
    text = PHONE_RE.sub(_phone_sub, text)

    if text != before:
        phone_count = text.count("[phone removed]")
        if phone_count > 0:
            score += 1.0 * phone_count

    return text, float(score)
